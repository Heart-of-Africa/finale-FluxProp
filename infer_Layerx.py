import torch
from LNNModel import LNNLanguageModel
from train_layer import CharTokenizer  # å‡è®¾ tokenizer åœ¨è®­ç»ƒæ–‡ä»¶ä¸­å®šä¹‰
import os

# åŠ è½½24ä¸ªæ¨¡å‹å±‚ï¼ˆå‡è®¾éƒ½ä¿å­˜åœ¨ checkpoints/layer_xx ç›®å½•ï¼‰
def load_merged_model(vocab_size, device="cuda"):
    num_layers = 24
    alpha = 0.1
    hidden_size = 256
    embed_size = 256

    model = LNNLanguageModel(
        vocab_size=vocab_size,
        embed_size=embed_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        alpha=alpha,
        device=device
    ).to(device)

    # æ‰‹åŠ¨åŠ è½½æ¯å±‚å‚æ•°
    for i in range(num_layers):
        path = f"./checkpoints/layer_{i:02d}/checkpoint_step_3000.pt"
        if os.path.exists(path):
            ckpt = torch.load(path, map_location=device)
            layer = model.liquid_layers[i]
            state = ckpt["model_state"]
            # åŠ è½½è¯¥å±‚çš„å‚æ•°ï¼Œå‡è®¾ key æ˜¯æœ‰å‘½åç©ºé—´çš„ï¼ˆä½ å¯èƒ½éœ€è¦åšæ˜ å°„ï¼‰
            for name, param in layer.named_parameters():
                full_key = f"liquid_layers.{i}.{name}"
                if full_key in state:
                    param.data.copy_(state[full_key])
                else:
                    print(f"[è­¦å‘Š] {full_key} ä¸åœ¨ checkpoint ä¸­")
        else:
            print(f"[è­¦å‘Š] ç¼ºå°‘ checkpointï¼š{path}")

    return model


# ç”Ÿæˆå‡½æ•°ï¼ˆå­—ç¬¦çº§ï¼‰
def generate(model, tokenizer, prompt, max_len=100):
    model.eval()
    device = next(model.parameters()).device
    input_ids = tokenizer.encode(prompt)
    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)
    output = prompt

    with torch.no_grad():
        for _ in range(max_len):
            logits = model(input_tensor)
            next_token = torch.argmax(logits[0]).item()
            output += tokenizer.decode([next_token])
            input_tensor = torch.cat([input_tensor[:, 1:], torch.tensor([[next_token]], device=device)], dim=1)

    return output


# ä¸»æµç¨‹
if __name__ == "__main__":
    with open("train.txt", "r", encoding="utf-8") as f:
        raw_text = f.read()

    tokenizer = CharTokenizer(raw_text)
    vocab_size = tokenizer.vocab_size()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = load_merged_model(vocab_size, device)

    prompt = "ä½ å¥½ï¼Œä»Šå¤©çš„å¤©æ°”"
    result = generate(model, tokenizer, prompt)
    print("ğŸ§  æ¨¡å‹ç”Ÿæˆï¼š", result)

